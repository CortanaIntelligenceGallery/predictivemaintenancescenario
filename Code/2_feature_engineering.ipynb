{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature Engineering\n",
    "\n",
    "Feature engineering combines the different data sources together to create a single data set of features (variables) that can be used to infer a machines's health condition over time. \n",
    "\n",
    "In this notebook, we will load the data stored in Azure Blob containers in the previous **Data Ingestion** notebook (`Code/1_data_ingestion.ipynb`). The note book uses several feature engineering methods to create a data set for use in our predictive maintenance machine learning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, unix_timestamp, round\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from Azure Blob storage container\n",
    "\n",
    "We have previously downloaded and stored the following data in an Azure blob storage container:\n",
    "\n",
    "  * Machines: Features differentiating each machine. For example age and model.\n",
    "  * Error: The log of non-critical errors. These errors may still indicate an impending component failure.\n",
    "  * Maint: Machine maintenance history detailing component replacement or regular maintenance activities withe the date of replacement.\n",
    "  * Telemetry: The operating conditions of a machine e.g. data collected from sensors.\n",
    "  * Failure history: The failure history of a machine or component within the machine.\n",
    "\n",
    "We'll load these files from blob, and create our analysis data set here. We'll write this data set back into a new blob container to use in our model building and evaluation notebook later. \n",
    "\n",
    "Since the Azure Blob storage account name and account key are not passed between notebooks, you'll need to provide those here again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your Azure blob storage details here \n",
    "ACCOUNT_NAME = \"<your blob storage account name>\"\n",
    "\n",
    "# You can find the account key under the _Access Keys_ link in the \n",
    "# [Azure Portal](portal.azure.com) page for your Azure storage container.\n",
    "ACCOUNT_KEY = \"<your blob storage account key>\"\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "# The data from the Data Aquisition note book is stored in the dataingestion container.\n",
    "CONTAINER_NAME = \"dataingestion\"\n",
    "\n",
    "# The data constructed in this notebook will be stored in the featureengineering container\n",
    "STORAGE_CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# Connect to your blob service     \n",
    "az_blob_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# We will store each of these data sets in blob storage in an \n",
    "# Azure Storage Container on your Azure subscription.\n",
    "# See https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md\n",
    "# for details.\n",
    "\n",
    "# These file names detail which blob each file is stored under. \n",
    "MACH_DATA = 'machines_files.parquet'\n",
    "MAINT_DATA = 'maint_files.parquet'\n",
    "ERROR_DATA = 'errors_files.parquet'\n",
    "TELEMETRY_DATA = 'telemetry_files.parquet'\n",
    "FAILURE_DATA = 'failure_files.parquet'\n",
    "\n",
    "# These file names detail the local paths where we store the data results.\n",
    "MACH_LOCAL_DIRECT = 'dataingestion_mach_result.parquet'\n",
    "ERROR_LOCAL_DIRECT = 'dataingestion_err_result.parquet'\n",
    "MAINT_LOCAL_DIRECT = 'dataingestion_maint_result.parquet'\n",
    "TELEMETRY_LOCAL_DIRECT = 'dataingestion_tel_result.parquet'\n",
    "FAILURES_LOCAL_DIRECT = 'dataingestion_fail_result.parquet'\n",
    "\n",
    "# This is the final data file.\n",
    "FEATURES_LOCAL_DIRECT = 'featureengineering_files.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machines data set\n",
    "\n",
    "Load the machines data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machineID</th>\n",
       "      <th>model</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>model2</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>model4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>model3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>model3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>model2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machineID   model  age\n",
       "0          1  model2   18\n",
       "1          2  model4    7\n",
       "2          3  model3    8\n",
       "3          4  model3    7\n",
       "4          5  model2    2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the entire parquet result folder to local path for a new run \n",
    "\n",
    "# create a local path  to store the data.\n",
    "if not os.path.exists(MACH_LOCAL_DIRECT):\n",
    "    os.makedirs(MACH_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# Connect to blob storage container\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if MACH_DATA in blob.name:\n",
    "        local_file = os.path.join(MACH_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "# Read in the data\n",
    "machines = spark.read.parquet(MACH_LOCAL_DIRECT)\n",
    "\n",
    "print(machines.count())\n",
    "machines.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors data set\n",
    "\n",
    "Load the errors data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "11967\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>machineID</th>\n",
       "      <th>errorID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-06 06:00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>error5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-06 06:00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>error1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-05-27 03:00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>error2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-08-19 06:00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>error2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-08-19 06:00:00</td>\n",
       "      <td>79</td>\n",
       "      <td>error3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  machineID errorID\n",
       "0  2015-04-06 06:00:00         79  error5\n",
       "1  2015-05-06 06:00:00         79  error1\n",
       "2  2015-05-27 03:00:00         79  error2\n",
       "3  2015-08-19 06:00:00         79  error2\n",
       "4  2015-08-19 06:00:00         79  error3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "\n",
    "# create a local path  to store the data.\n",
    "if not os.path.exists(ERROR_LOCAL_DIRECT):\n",
    "    os.makedirs(ERROR_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# Connect to blob storage container\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if ERROR_DATA in blob.name:\n",
    "        local_file = os.path.join(ERROR_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "# Read in the data\n",
    "errors = spark.read.parquet(ERROR_LOCAL_DIRECT)\n",
    "\n",
    "print(errors.count())\n",
    "errors.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maintenance data set\n",
    "\n",
    "Load the maintenance data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "32592\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>machineID</th>\n",
       "      <th>comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-05-31 06:00:00</td>\n",
       "      <td>125</td>\n",
       "      <td>comp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-05-31 06:00:00</td>\n",
       "      <td>125</td>\n",
       "      <td>comp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-15 06:00:00</td>\n",
       "      <td>125</td>\n",
       "      <td>comp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-15 06:00:00</td>\n",
       "      <td>125</td>\n",
       "      <td>comp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-15 06:00:00</td>\n",
       "      <td>125</td>\n",
       "      <td>comp2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  machineID   comp\n",
       "0  2015-05-31 06:00:00        125  comp3\n",
       "1  2015-05-31 06:00:00        125  comp4\n",
       "2  2015-06-15 06:00:00        125  comp4\n",
       "3  2015-06-15 06:00:00        125  comp3\n",
       "4  2015-07-15 06:00:00        125  comp2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "\n",
    "# create a local path  to store the data.\n",
    "if not os.path.exists(MAINT_LOCAL_DIRECT):\n",
    "    os.makedirs(MAINT_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# Connect to blob storage container\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if MAINT_DATA in blob.name:\n",
    "        local_file = os.path.join(MAINT_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "# Read in the data\n",
    "maint = spark.read.parquet(MAINT_LOCAL_DIRECT)\n",
    "\n",
    "print(maint.count())\n",
    "maint.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telemetry\n",
    "\n",
    "Load the telemetry data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "8761000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>machineID</th>\n",
       "      <th>volt</th>\n",
       "      <th>rotate</th>\n",
       "      <th>pressure</th>\n",
       "      <th>vibration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-12-19 22:00:00</td>\n",
       "      <td>625</td>\n",
       "      <td>138.923911</td>\n",
       "      <td>332.555602</td>\n",
       "      <td>99.460533</td>\n",
       "      <td>43.493783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-12-19 23:00:00</td>\n",
       "      <td>625</td>\n",
       "      <td>173.839769</td>\n",
       "      <td>335.473306</td>\n",
       "      <td>103.150583</td>\n",
       "      <td>46.760175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-12-20 00:00:00</td>\n",
       "      <td>625</td>\n",
       "      <td>177.709243</td>\n",
       "      <td>456.502368</td>\n",
       "      <td>94.547367</td>\n",
       "      <td>42.063502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-12-20 01:00:00</td>\n",
       "      <td>625</td>\n",
       "      <td>176.464697</td>\n",
       "      <td>438.722509</td>\n",
       "      <td>106.360646</td>\n",
       "      <td>36.449788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-12-20 02:00:00</td>\n",
       "      <td>625</td>\n",
       "      <td>177.080815</td>\n",
       "      <td>407.972662</td>\n",
       "      <td>92.472901</td>\n",
       "      <td>33.644790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  machineID        volt      rotate    pressure  \\\n",
       "0  2015-12-19 22:00:00        625  138.923911  332.555602   99.460533   \n",
       "1  2015-12-19 23:00:00        625  173.839769  335.473306  103.150583   \n",
       "2  2015-12-20 00:00:00        625  177.709243  456.502368   94.547367   \n",
       "3  2015-12-20 01:00:00        625  176.464697  438.722509  106.360646   \n",
       "4  2015-12-20 02:00:00        625  177.080815  407.972662   92.472901   \n",
       "\n",
       "   vibration  \n",
       "0  43.493783  \n",
       "1  46.760175  \n",
       "2  42.063502  \n",
       "3  36.449788  \n",
       "4  33.644790  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "\n",
    "# create a local path  to store the data.\n",
    "if not os.path.exists(TELEMETRY_LOCAL_DIRECT):\n",
    "    os.makedirs(TELEMETRY_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# Connect to blob storage container\n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if TELEMETRY_DATA in blob.name:\n",
    "        local_file = os.path.join(TELEMETRY_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "# Read in the data\n",
    "telemetry = spark.read.parquet(TELEMETRY_LOCAL_DIRECT)\n",
    "\n",
    "print(telemetry.count())\n",
    "telemetry.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failures data set\n",
    "\n",
    "Load the failures data set from your Azure blob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "6726\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>machineID</th>\n",
       "      <th>failure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-08-30 06:00:00</td>\n",
       "      <td>498</td>\n",
       "      <td>comp1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-11-13 06:00:00</td>\n",
       "      <td>498</td>\n",
       "      <td>comp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-23 06:00:00</td>\n",
       "      <td>499</td>\n",
       "      <td>comp2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-22 06:00:00</td>\n",
       "      <td>499</td>\n",
       "      <td>comp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-03-24 06:00:00</td>\n",
       "      <td>499</td>\n",
       "      <td>comp2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  machineID failure\n",
       "0  2015-08-30 06:00:00        498   comp1\n",
       "1  2015-11-13 06:00:00        498   comp4\n",
       "2  2015-01-23 06:00:00        499   comp2\n",
       "3  2015-02-22 06:00:00        499   comp3\n",
       "4  2015-03-24 06:00:00        499   comp2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the previous created final dataset into the workspace\n",
    "\n",
    "# create a local path  to store the data.\n",
    "if not os.path.exists(FAILURES_LOCAL_DIRECT):\n",
    "    os.makedirs(FAILURES_LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in az_blob_service.list_blobs(CONTAINER_NAME):\n",
    "    if FAILURE_DATA in blob.name:\n",
    "        local_file = os.path.join(FAILURES_LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        az_blob_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "failures = spark.read.parquet(FAILURES_LOCAL_DIRECT)\n",
    "\n",
    "print(failures.count())\n",
    "failures.limit(5).toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "\n",
    "Feature engineering combines the different data sources together to create a single data set of features (variables) that can be used to infer a machines's health condition over time. The ultimate goal is to generate a single record for each time unit for each asset combining its features and labels to be fed into the machine learning algorithm. In order to prepare that clean final data set, some pre-processing steps should be taken. First step is to divide the duration of data collection into time units where each record belongs to a time unit for an asset.\n",
    "\n",
    "The measurement unit for time can be in seconds, minutes, hours, days, months, cycles, miles or transactions depending on the efficiency of data preparation and the changes observed in the conditions of the asset from a time unit to the other or other factors specific to the domain. In other words, the time unit does not have to be the same as the frequency of data collection as in many cases data may not show any difference from one unit to the other. For example, if temperature values were being collected every 10 seconds, picking a time unit of 10 seconds for the whole analysis inflates the number of examples without providing any additional information. Better strategy would be to use average over an hour as an example.\n",
    "\n",
    "### Rolling aggregates\n",
    "\n",
    "For each record of an asset, we pick a rolling window of size \"W\" which is the number of units of time that we would like to compute historical aggregates for. We then compute rolling aggregate features using the W periods before the date of that record. Some example rolling aggregates can be rolling counts, means, standard deviations, outliers based on standard deviations, CUSUM measures, minimum and maximum values for the window. Another interesting technique is to capture trend changes, spikes and level changes using algorithms that detect anomalies in data using anomaly detection algorithms.\n",
    "\n",
    "### Lag features\n",
    "As mentioned earlier, in predictive maintenance, historical data usually comes with timestamps indicating the time of collection for each piece of data. There are many ways of creating features from the data that comes with timestamped data. In this section, we discuss some of these methods used for predictive maintenance. However, we are not limited by these methods alone. Since feature engineering is considered to be one of the most creative areas of predictive modeling, there could be many other ways to create features. Here, we provide some general techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telemetry features\n",
    "\n",
    "Because the telemetry data set is the largest time series data we have, we start feature engineering here. \n",
    "\n",
    "A common method is to pick a window size for the lag features to be created and compute rolling aggregate measures such as mean, standard deviation, minimum, maximum, etc. to represent the short term history of the telemetry over the window. In the following, rolling mean and standard deviation of the telemetry data over the last 3 hour and 24 hour lag windows is calculated for every 3 hours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 60 ms, sys: 20 ms, total: 80 ms\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# rolling mean\n",
    "# Temporary storage for rolling means\n",
    "tel_mean = telemetry\n",
    "\n",
    "# Which features are we interested in telemetry data set\n",
    "rolling_features = ['volt','rotate', 'pressure', 'vibration']\n",
    "               \n",
    "# We choose two windows for our rolling windows 3hrs, 24 hrs\n",
    "lags = [3,24]\n",
    "\n",
    "for lag_n in lags:\n",
    "    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n",
    "    for col_name in rolling_features:\n",
    "        tel_mean = tel_mean.withColumn(col_name+'_rollingmean_'+str(lag_n), F.avg(col(col_name)).over(wSpec))\n",
    "\n",
    "# 3 hours = 10800 seconds  \n",
    "time_val = 3 * 60 * 60\n",
    "\n",
    "# I think this is grabbing datetime from the tel_sd data set, which is equivalent to telemetry\n",
    "dt_truncated = ((round(unix_timestamp(col(\"datetime\")) / time_val) * time_val).cast(\"timestamp\"))\n",
    "\n",
    "tel_mean_resampled = tel_mean.withColumn(\"dt_truncated\", dt_truncated).drop('volt', 'rotate', 'pressure', 'vibration')\n",
    "\n",
    "tel_mean = (tel_mean_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "            .agg(F.mean('volt_rollingmean_3').alias('volt_rollingmean_3'),\n",
    "                 F.mean('rotate_rollingmean_3').alias('rotate_rollingmean_3'), \n",
    "                 F.mean('pressure_rollingmean_3').alias('pressure_rollingmean_3'), \n",
    "                 F.mean('vibration_rollingmean_3').alias('vibration_rollingmean_3'), \n",
    "                 F.mean('volt_rollingmean_24').alias('volt_rollingmean_24'),\n",
    "                 F.mean('rotate_rollingmean_24').alias('rotate_rollingmean_24'), \n",
    "                 F.mean('pressure_rollingmean_24').alias('pressure_rollingmean_24'), \n",
    "                 F.mean('vibration_rollingmean_24').alias('vibration_rollingmean_24')))\n",
    "\n",
    "# rolling standard deviation\n",
    "# Temporary storage for rolling means\n",
    "tel_sd = telemetry\n",
    "\n",
    "for lag_n in lags:\n",
    "    wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-lag_n, 0)\n",
    "    for col_name in rolling_features:\n",
    "        tel_sd = tel_sd.withColumn(col_name+'_rollingstd_'+str(lag_n), F.stddev(col(col_name)).over(wSpec))\n",
    "    \n",
    "# tel_sd rolling sd\n",
    "tel_sd_resampled = (tel_sd.withColumn(\"dt_truncated\", dt_truncated)\n",
    "                    .drop('volt', 'rotate', 'pressure', 'vibration')\n",
    "                    .fillna(0))\n",
    "\n",
    "tel_sd = (tel_sd_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "          .agg(F.stddev('volt_rollingstd_3').alias('volt_rollingstd_3'),\n",
    "               F.stddev('rotate_rollingstd_3').alias('rotate_rollingstd_3'), \n",
    "               F.stddev('pressure_rollingstd_3').alias('pressure_rollingstd_3'), \n",
    "               F.stddev('vibration_rollingstd_3').alias('vibration_rollingstd_3'), \n",
    "               F.stddev('volt_rollingstd_24').alias('volt_rollingstd_24'),\n",
    "               F.stddev('rotate_rollingstd_24').alias('rotate_rollingstd_24'), \n",
    "               F.stddev('pressure_rollingstd_24').alias('pressure_rollingstd_24'), \n",
    "               F.stddev('vibration_rollingstd_24').alias('vibration_rollingstd_24')))\n",
    "\n",
    "telemetry_feat = (tel_mean.join(tel_sd,\n",
    "                                ((tel_mean['machineID'] == tel_sd['machineID']) \n",
    "                                 & (tel_mean['dt_truncated'] == tel_sd['dt_truncated'])), \n",
    "                                \"left\")\n",
    "                  .drop(tel_sd.machineID).drop(tel_sd.dt_truncated))\n",
    "\n",
    "telemetry_feat.where((col(\"machineID\") == 1)).limit(10).toPandas().head(10)\n",
    "\n",
    "# Clean up temp memory space\n",
    "tel_mean_resampled.unpersist()\n",
    "tel_sd_resampled.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors features\n",
    "\n",
    "Like telemetry data, errors come with timestamps. An important difference is that the error IDs are categorical values and should not be averaged over time intervals like the telemetry measurements. Instead, we count the number of errors of each type in a lagging window. We begin by reformatting the error data to have one entry per machine per time at which at least one error occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 8 ms, total: 40 ms\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a column for each errorID \n",
    "error1 = errors.groupBy(\"machineID\",\"datetime\",\"errorID\").pivot('errorID').agg(F.count('machineID').alias('dummy'))\n",
    "\n",
    "# remove the column called errorID and fill in missing values\n",
    "error2 = error1.drop('errorID').fillna(0)\n",
    "\n",
    "# combine errors for a given machine in a given hour\n",
    "error3 = (error2.groupBy(\"machineID\",\"datetime\")\n",
    "                .agg(F.sum('error1').alias('error1sum'), \n",
    "                     F.sum('error2').alias('error2sum'), \n",
    "                     F.sum('error3').alias('error3sum'), \n",
    "                     F.sum('error4').alias('error4sum'), \n",
    "                     F.sum('error5').alias('error5sum')))\n",
    "\n",
    "# join the telemetry data with errors\n",
    "error_count = (telemetry.join(error3, ((telemetry['machineID'] == error3['machineID']) \n",
    "                                       & (telemetry['datetime'] == error3['datetime'])), \"left\")\n",
    "               .drop('volt', 'rotate', 'pressure', 'vibration').drop(error3.machineID).drop(error3.datetime))\n",
    "\n",
    "# fill in missing value\n",
    "err_mean = error_count.fillna(0)\n",
    "\n",
    "# remove \n",
    "error_count.unpersist()\n",
    "error1.unpersist()\n",
    "error2.unpersist()\n",
    "error3.unpersist()\n",
    "\n",
    "error_features = ['error1sum','error2sum', 'error3sum', 'error4sum', 'error5sum']\n",
    "\n",
    "wSpec = Window.partitionBy('machineID').orderBy('datetime').rowsBetween(1-24, 0)\n",
    "for col_name in error_features:\n",
    "    err_mean = err_mean.withColumn(col_name+'_rollingmean_'+str(lag_n), F.avg(col(col_name)).over(wSpec))\n",
    "\n",
    "dt_truncated = ((round(unix_timestamp(col(\"datetime\").cast(\"timestamp\")) / time_val) * time_val)\n",
    "    .cast(\"timestamp\"))\n",
    "\n",
    "err_mean_resampled = (err_mean.withColumn(\"dt_truncated\", dt_truncated)\n",
    "                    .drop('error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum').fillna(0))\n",
    "\n",
    "err_feat = (err_mean_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "            .agg(F.mean('error1sum_rollingmean_24').alias('error1sum_rollingmean_24'), \n",
    "                 F.mean('error2sum_rollingmean_24').alias('error2sum_rollingmean_24'), \n",
    "                 F.mean('error3sum_rollingmean_24').alias('error3sum_rollingmean_24'), \n",
    "                 F.mean('error4sum_rollingmean_24').alias('error4sum_rollingmean_24'), \n",
    "                 F.mean('error5sum_rollingmean_24').alias('error5sum_rollingmean_24')))\n",
    "\n",
    "err_feat.count(), len(err_mean.columns)\n",
    "err_feat.limit(10).toPandas().head(10)\n",
    "\n",
    "# Clean up temp memory space\n",
    "err_mean_resampled.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement from maintenance \n",
    "\n",
    "A crucial data set in this example is the maintenance records which contain the information of component replacement records. Possible features from this data set can be, for example, the number of replacements of each component in the last 3 months to incorporate the frequency of replacements. However, more relevent information would be to calculate how long it has been since a component is last replaced as that would be expected to correlate better with component failures since the longer a component is used, the more degradation should be expected.\n",
    "\n",
    "As a side note, creating lagging features from maintenance data is not as straightforward as for telemetry and errors, so the features from this data are generated in a more custom way. This type of ad-hoc feature engineering is very common in predictive maintenance since domain knowledge plays a big role in understanding the predictors of a problem. In the following, the days since last component replacement are calculated for each component type as features from the maintenance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create a column for each comp \n",
    "maint1 = maint.groupBy(\"machineID\",\"datetime\",\"comp\").pivot('comp').agg(F.count('machineID').alias('dummy'))\n",
    "\n",
    "# remove the column called comp and fill in missing values\n",
    "maint2 = maint1.drop('comp').fillna(0)\n",
    "\n",
    "# combine maintenance for a given machine in a given hour\n",
    "maint_replace = (maint2.groupBy(\"machineID\",\"datetime\")\n",
    "                 .agg(F.sum('comp1').alias('comp1sum'), \n",
    "                      F.sum('comp2').alias('comp2sum'), \n",
    "                      F.sum('comp3').alias('comp3sum'),\n",
    "                      F.sum('comp4').alias('comp4sum')))\n",
    "\n",
    "maint_replace.limit(10).toPandas().head(10)\n",
    "\n",
    "# Remove temporary dataframes\n",
    "maint1.unpersist()\n",
    "maint2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# We want to align the component information on telemetry timestamps.\n",
    "telemetry_times = (telemetry.withColumnRenamed('datetime','datetime_tel')\n",
    "                   .drop(telemetry.volt).drop(telemetry.rotate).drop(telemetry.pressure).drop(telemetry.vibration))\n",
    "\n",
    "# Get the counts of comp1 replacement.\n",
    "maint_comp1 = (maint_replace.where((col(\"comp1sum\") == '1'))\n",
    "               .withColumnRenamed('datetime','datetime_maint')\n",
    "               .drop('comp2sum', 'comp3sum', 'comp4sum'))\n",
    "\n",
    "# Align on telemetry time\n",
    "maint_tel_comp1 = telemetry_times.join(maint_comp1, ((telemetry_times['machineID']==maint_comp1['machineID']) \n",
    "                                                     & (telemetry_times['datetime_tel'] > maint_comp1['datetime_maint']) \n",
    "                                                     & (maint_comp1['comp1sum'] == '1'))).drop(maint_comp1.machineID)\n",
    "\n",
    "# Calculate the days since comp1 replacement.\n",
    "comp1 = (maint_tel_comp1.withColumn(\"sincelastcomp1\", \n",
    "                                    datediff(maint_tel_comp1.datetime_tel, maint_tel_comp1.datetime_maint))\n",
    "         .drop(maint_tel_comp1.datetime_maint).drop(maint_tel_comp1.comp1sum))\n",
    "\n",
    "comp1.limit(5).toPandas().head(5)\n",
    "\n",
    "# Remove temporary dataframes\n",
    "maint_tel_comp1.unpersist()\n",
    "maint_comp1.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 3.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "maint_comp2 = (maint_replace.where(col(\"comp2sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n",
    "               .drop('comp1sum', 'comp3sum', 'comp4sum'))\n",
    "\n",
    "maint_tel_comp2 = (telemetry_times.join(maint_comp2, \n",
    "                                        ((telemetry_times ['machineID']== maint_comp2['machineID']) \n",
    "                                         & (telemetry_times ['datetime_tel'] > maint_comp2['datetime_maint']) \n",
    "                                         & ( maint_comp2['comp2sum'] == '1')))\n",
    "                   .drop(maint_comp2.machineID))\n",
    "\n",
    "comp2 = (maint_tel_comp2.withColumn(\"sincelastcomp2\", \n",
    "                                    datediff(maint_tel_comp2.datetime_tel, maint_tel_comp2.datetime_maint))\n",
    "         .drop(maint_tel_comp2.datetime_maint).drop(maint_tel_comp2.comp2sum))\n",
    "\n",
    "comp2.limit(5).toPandas().head(5)\n",
    "\n",
    "# Remove temporary dataframes\n",
    "maint_tel_comp2.unpersist()\n",
    "maint_comp2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "maint_comp3 = (maint_replace.where(col(\"comp3sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n",
    "               .drop('comp1sum', 'comp2sum', 'comp4sum'))\n",
    "\n",
    "maint_tel_comp3 = (telemetry_times.join(maint_comp3, ((telemetry_times ['machineID']==maint_comp3['machineID']) \n",
    "                                                      & (telemetry_times ['datetime_tel'] > maint_comp3['datetime_maint']) \n",
    "                                                      & ( maint_comp3['comp3sum'] == '1')))\n",
    "                   .drop(maint_comp3.machineID))\n",
    "\n",
    "comp3 = (maint_tel_comp3.withColumn(\"sincelastcomp3\", \n",
    "                                    datediff(maint_tel_comp3.datetime_tel, maint_tel_comp3.datetime_maint))\n",
    "         .drop(maint_tel_comp3.datetime_maint).drop(maint_tel_comp3.comp3sum))\n",
    "\n",
    "comp3.limit(5).toPandas().head(5)\n",
    "\n",
    "# Remove temporary dataframes\n",
    "maint_tel_comp3.unpersist()\n",
    "maint_comp3.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days since last replacement for component-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 3.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "maint_comp4 = (maint_replace.where(col(\"comp4sum\") == '1').withColumnRenamed('datetime','datetime_maint')\n",
    "               .drop('comp1sum', 'comp2sum', 'comp3sum'))\n",
    "\n",
    "maint_tel_comp4 = telemetry_times.join(maint_comp4, ((telemetry_times['machineID']==maint_comp4['machineID']) \n",
    "                                                     & (telemetry_times['datetime_tel'] > maint_comp4['datetime_maint']) \n",
    "                                                     & (maint_comp4['comp4sum'] == '1'))).drop(maint_comp4.machineID)\n",
    "\n",
    "comp4 = (maint_tel_comp4.withColumn(\"sincelastcomp4\", \n",
    "                                    datediff(maint_tel_comp4.datetime_tel, maint_tel_comp4.datetime_maint))\n",
    "         .drop(maint_tel_comp4.datetime_maint).drop(maint_tel_comp4.comp4sum))\n",
    "\n",
    "comp4.limit(20).toPandas().head(20)\n",
    "\n",
    "# Remove temporary dataframes\n",
    "maint_tel_comp4.unpersist()\n",
    "maint_comp4.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Combine comp1, comp2, comp3, comp4 to generate the maintenance feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 132 ms, sys: 32 ms, total: 164 ms\n",
      "Wall time: 7min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# left join comp1 with (comp2, comp3, comp4) \n",
    "# left join comp2 with (comp3, comp4) \n",
    "# left join comp3, comp4 \n",
    "comp3_4 = (comp3.join(comp4, ((comp3['machineID'] == comp4['machineID']) \n",
    "                              & (comp3['datetime_tel'] == comp4['datetime_tel'])), \"left\")\n",
    "           .drop(comp4.machineID).drop(comp4.datetime_tel))\n",
    "\n",
    "comp2_3_4 = (comp2.join(comp3_4, ((comp2['machineID'] == comp3_4['machineID']) \n",
    "                                  & (comp2['datetime_tel'] == comp3_4['datetime_tel'])), \"left\")\n",
    "             .drop(comp3_4.machineID).drop(comp3_4.datetime_tel))\n",
    "\n",
    "comp1_2_3_4 = (comp1.join(comp2_3_4, ((comp1['machineID'] == comp2_3_4['machineID']) \n",
    "                                      & (comp1['datetime_tel'] == comp2_3_4['datetime_tel'])), \"left\")\n",
    "               .drop(comp2_3_4.machineID).drop(comp2_3_4.datetime_tel))\n",
    "\n",
    "comp1_2_3_4_final = (comp1_2_3_4.groupBy(\"machineID\", \"datetime_tel\")\n",
    "                     .agg(F.max('sincelastcomp1').alias('sincelastcomp1'), \n",
    "                          F.max('sincelastcomp2').alias('sincelastcomp2'), \n",
    "                          F.max('sincelastcomp3').alias('sincelastcomp3'), \n",
    "                          F.max('sincelastcomp4').alias('sincelastcomp4')))\n",
    "\n",
    "# fill in missing value\n",
    "maint_count = comp1_2_3_4_final.fillna(0)\n",
    "\n",
    "\n",
    "# maint_count1 maintenance \n",
    "dt_truncated = ((round(unix_timestamp(col(\"datetime_tel\").cast(\"timestamp\")) / time_val) * time_val)\n",
    "                .cast(\"timestamp\"))\n",
    "\n",
    "maint_resampled = maint_count.withColumn(\"dt_truncated\", dt_truncated)\n",
    "maint_feat = (maint_resampled.groupBy(\"machineID\",\"dt_truncated\")\n",
    "              .agg(F.mean('sincelastcomp1').alias('comp1sum'), \n",
    "                   F.mean('sincelastcomp2').alias('comp2sum'), \n",
    "                   F.mean('sincelastcomp3').alias('comp3sum'), \n",
    "                   F.mean('sincelastcomp4').alias('comp4sum')))\n",
    "\n",
    "\n",
    "maint_feat.limit(10).toPandas().head(10)\n",
    "\n",
    "# Remove temporary dataframes\n",
    "comp1_2_3_4_final.unpersist()\n",
    "comp1_2_3_4.unpersist()\n",
    "comp2_3_4.unpersist()\n",
    "comp3_4.unpersist()\n",
    "comp1.unpersist()\n",
    "comp2.unpersist()\n",
    "comp3.unpersist()\n",
    "comp4.unpersist()\n",
    "maint_replace.unpersist()\n",
    "maint_resampled.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine features\n",
    "\n",
    "The machine features can be used without further modification. These include descriptive information about the type of each machine and its age (number of years in service). If the age information had been recorded as a \"first use date\" for each machine, a transformation would have been necessary to turn those into a numeric values indicating the years in service.\n",
    "\n",
    "We do need to create a set of dummy features, boolean variables to indicate the model name of the machine. This is a _one-hot encoding_ step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 4 ms, total: 16 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# one hot encoding of the variable model\n",
    "catVarNames = ['model']  \n",
    "sIndexers = [StringIndexer(inputCol=x, outputCol=x + '_indexed') for x in catVarNames]\n",
    "machines_cat = Pipeline(stages=sIndexers).fit(machines).transform(machines)\n",
    "\n",
    "# one-hot encode\n",
    "ohEncoders = [OneHotEncoder(inputCol=x + '_indexed', outputCol=x + '_encoded')\n",
    "              for x in catVarNames]\n",
    "\n",
    "ohPipelineModel = Pipeline(stages=ohEncoders).fit(machines_cat)\n",
    "machines_cat = ohPipelineModel.transform(machines_cat)\n",
    "\n",
    "drop_list = [col_n for col_n in machines_cat.columns if 'indexed' in col_n]\n",
    "\n",
    "machines_feat = machines_cat.select([column for column in machines_cat.columns if column not in drop_list])\n",
    "\n",
    "machines_feat.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data into feature engineering set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 8 ms, total: 172 ms\n",
      "Wall time: 8min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# join error features with component maintenance features\n",
    "error_maint = (err_feat.join(maint_feat, \n",
    "                             ((err_feat['machineID'] == maint_feat['machineID']) \n",
    "                              & (err_feat['dt_truncated'] == maint_feat['dt_truncated'])), \"left\")\n",
    "               .drop(maint_feat.machineID).drop(maint_feat.dt_truncated))\n",
    "\n",
    "# now join with machines features\n",
    "err_maint_mach = (error_maint.join(machines_feat, \n",
    "                                   ((error_maint['machineID'] == machines_feat['machineID'])), \"left\")\n",
    "                  .drop(machines_feat.machineID))\n",
    "\n",
    "\n",
    "err_maint_mach_feat = (err_maint_mach.select([c for c in err_maint_mach.columns if c not in \n",
    "                                              {'error1sum', 'error2sum', 'error3sum', 'error4sum', 'error5sum'}]))\n",
    "\n",
    "# join telemetry_all with err_maint_mach_select to create final feature matrix\n",
    "final_feat = (telemetry_feat.join(err_maint_mach_feat, \n",
    "                                  ((telemetry_feat['machineID'] == err_maint_mach_feat['machineID']) \n",
    "                                   & (telemetry_feat['dt_truncated'] == err_maint_mach_feat['dt_truncated'])), \"left\")\n",
    "              .drop(err_maint_mach_feat.machineID).drop(err_maint_mach_feat.dt_truncated))\n",
    "\n",
    "err_feat.unpersist()\n",
    "maint_feat.unpersist()\n",
    "machines_feat.unpersist()\n",
    "telemetry_feat.unpersist()\n",
    "error_maint.unpersist()\n",
    "err_maint_mach.unpersist()\n",
    "err_maint_mach_feat.unpersist()\n",
    "\n",
    "final_feat.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label construction\n",
    "\n",
    "When using multi-class classification for predicting failure due to a problem, labelling is done by taking a time window prior to the failure of an asset and labelling the feature records that fall into that window as \"about to fail due to a problem\" while labelling all other records as \"Â€Âœnormal.\" This time window should be picked according to the business case: in some situations it may be enough to predict failures hours in advance, while in others days or weeks may be needed to allow e.g. for arrival of replacement parts.\n",
    "\n",
    "The prediction problem for this example scenerio is to estimate the probability that a machine will fail in the near future due to a failure of a certain component. More specifically, the goal is to compute the probability that a machine will fail in the next 24 hours due to a certain component failure (component 1, 2, 3, or 4). Below, a categorical failure feature is created to serve as the label. All records within a 24 hour window before a failure of component 1 have failure=comp1, and so on for components 2, 3, and 4; all records not within 24 hours of a component failure have failure=none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# check to see if there are duplicate rows based on machine, datetime\n",
    "failures1 = failures.dropDuplicates(['machineID', 'datetime'])\n",
    "\n",
    "# map the failure data to final feature matrix\n",
    "labeled_features = (final_feat.join(failures1, ((final_feat['machineID'] == failures1['machineID']) \n",
    "                                                & (final_feat['dt_truncated'] == failures1['datetime'])), \"left\")\n",
    "                    .drop(failures1.machineID).drop(failures1.datetime))\n",
    "\n",
    "# recoding the column 'failure' to be numeric double for the pyspark classification models\n",
    "labeled_features1 = (labeled_features\n",
    "                     .withColumn('failure', F.when(col('failure') == \"comp1\", 1.0).otherwise(col('failure')))\n",
    "                     .withColumn('failure', F.when(col('failure') == \"comp2\", 2.0).otherwise(col('failure')))\n",
    "                     .withColumn('failure', F.when(col('failure') == \"comp3\", 3.0).otherwise(col('failure')))\n",
    "                     .withColumn('failure', F.when(col('failure') == \"comp4\", 4.0).otherwise(col('failure'))))\n",
    "\n",
    "labeled_features2 = labeled_features1.withColumn(\"failure1\", labeled_features1[\"failure\"].cast(DoubleType()))\n",
    "labeled_feat = labeled_features2.drop('failure').fillna(0)\n",
    "\n",
    "failures1.unpersist()\n",
    "labeled_features2.unpersist()\n",
    "labeled_features1.unpersist()\n",
    "final_feat.unpersist()\n",
    "\n",
    "labeled_feat.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels are created as 7 days before the actual failure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# lag values to manually backfill label (bfill =7)\n",
    "my_window = Window.partitionBy('machineID').orderBy(labeled_feat.dt_truncated.desc())\n",
    "\n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value1\", F.lag(labeled_feat.failure1).over(my_window)).fillna(0)\n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value2\", F.lag(labeled_feat.prev_value1).over(my_window)).fillna(0) \n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value3\", F.lag(labeled_feat.prev_value2).over(my_window)).fillna(0) \n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value4\", F.lag(labeled_feat.prev_value3).over(my_window)).fillna(0) \n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value5\", F.lag(labeled_feat.prev_value4).over(my_window)).fillna(0) \n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value6\", F.lag(labeled_feat.prev_value5).over(my_window)).fillna(0) \n",
    "labeled_feat = labeled_feat.withColumn(\"prev_value7\", F.lag(labeled_feat.prev_value6).over(my_window)).fillna(0)\n",
    "\n",
    "# create the label column \n",
    "label_bfill2 = (labeled_feat.withColumn('label', labeled_feat.failure1 + labeled_feat.prev_value1 \n",
    "                                        + labeled_feat.prev_value2 + labeled_feat.prev_value3 \n",
    "                                        + labeled_feat.prev_value4 + labeled_feat.prev_value5 \n",
    "                                        + labeled_feat.prev_value6 + labeled_feat.prev_value7))\n",
    "\n",
    "label_bfill2 = label_bfill2.withColumn('label_e', F.when(col('label') > 4, 4.0).otherwise(col('label')))\n",
    "\n",
    "labeled_feat = (label_bfill2.drop(label_bfill2.prev_value1).drop(label_bfill2.prev_value2)\n",
    "                .drop(label_bfill2.prev_value3).drop(label_bfill2.prev_value4)\n",
    "                .drop(label_bfill2.prev_value5).drop(label_bfill2.prev_value6)\n",
    "                .drop(label_bfill2.prev_value7).drop(label_bfill2.label))\n",
    "\n",
    "labeled_feat.count()\n",
    "labeled_feat.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Write the feature data to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# write the final result as parquet file in blob location \n",
    "# https://github.com/Azure/ViennaDocs/blob/master/Documentation/UsingBlobForStorage.md\n",
    "\n",
    "# Create a new container if necessary, otherwise you can use an existing container.\n",
    "# This command creates the container if it does not already exist. Else it does nothing.\n",
    "az_blob_service.create_container(STORAGE_CONTAINER_NAME, \n",
    "                                 fail_on_exist=False, \n",
    "                                 public_access=PublicAccess.Container)\n",
    "\n",
    "# you decide to partition the dataframe into three files and save them in the current folder.\n",
    "# if you wish to visualize them in the run history Output Files, specify the path \n",
    "# as './outputs/multiple_files.parquet'.\n",
    "#label_bfill3.coalesce(3).write.mode('overwrite').parquet('multiple_files.parquet')\n",
    "labeled_feat.write.mode('overwrite').parquet(FEATURES_LOCAL_DIRECT)\n",
    "\n",
    "# unlike the single file case, for multiple files we need to first delete results from the \n",
    "# previous run before uploading.\n",
    "for blob in az_blob_service.list_blobs(STORAGE_CONTAINER_NAME):\n",
    "    if FEATURES_LOCAL_DIRECT in blob.name:\n",
    "        az_blob_service.delete_blob(STORAGE_CONTAINER_NAME, blob.name)\n",
    "\n",
    "# upload the entire folder into blob storage\n",
    "for name in glob.iglob(FEATURES_LOCAL_DIRECT + '/*'):\n",
    "    print(os.path.abspath(name))\n",
    "    az_blob_service.create_blob_from_path(STORAGE_CONTAINER_NAME, name, name)\n",
    "\n",
    "print(\"Feature engineering final dataset files saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "The next step is to build and compare machine learning models using the feature data set we have just created. The `Code\\3_model_building.ipynb` notebook works through building a Decision Tree Classifier and a Random Forest Classifier using this data set. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hack2 myDockerVm",
   "language": "python",
   "name": "hack2_mydockervm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
