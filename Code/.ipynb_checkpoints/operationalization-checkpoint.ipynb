{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Model operationalization & Deployment\n",
    "\n",
    "In this script, a model is saved as a .model file along with the relevant scheme for deployment. The functions are first tested locally before operationalizing the model using Azure Machine Learning Model Management environment for use in production in realtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## setup our environment by importing required libraries\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azure.storage.blob import PublicAccess\n",
    "\n",
    "# for creating pipelines and model\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# setup the pyspark environment\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE creating a local directory!\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "|machineID|        dt_truncated|volt_rollingmean_3|rotate_rollingmean_3|pressure_rollingmean_3|vibration_rollingmean_3|volt_rollingmean_24|rotate_rollingmean_24|pressure_rollingmean_24|vibration_rollingmean_24| volt_rollingstd_3|rotate_rollingstd_3|pressure_rollingstd_3|vibration_rollingstd_3|volt_rollingstd_24|rotate_rollingstd_24|pressure_rollingstd_24|vibration_rollingstd_24|error1sum_rollingmean_24|error2sum_rollingmean_24|error3sum_rollingmean_24|error4sum_rollingmean_24|error5sum_rollingmean_24|         comp1sum|         comp2sum|         comp3sum|         comp4sum| model|age|model_encoded|failure1|label_e|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "|      114|2016-01-01 06:00:...| 163.3757329023745|  333.14948458556535|    100.18395169796328|     44.095881263819514| 164.11472399129667|     277.191815231867|      97.62891107072754|       50.88535051605059|21.004956521854176|  67.52872593782637|   12.936152686125933|     4.613597609179632|15.537773806210083|   67.65198854409817|     10.52827463297973|      6.941294875549091|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            489.0|            549.0|            549.0|            564.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2016-01-01 03:00:...| 168.0560723838507|  279.96957534682707|     97.63372820912399|      48.24221679536314| 164.03977789009616|    265.4140301347857|       98.2692713270434|      51.574366655467436|17.820643824484396| 49.637429003930606|    9.789304756066265|     7.963344413644408| 15.48856269342518|   67.26398825199145|    10.343146088525474|      6.738109627670492|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            489.0|            549.0|            549.0|            564.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2016-01-01 00:00:...|161.59623278663096|   277.7171441565807|     95.71420759202182|     51.508182534644625| 164.56778047540993|   265.99342004784285|       99.6572121485503|       50.46248490865013|20.582267478545578|  80.97739710207937|    8.128630498837182|     10.04603679663133|15.551165354528889|   65.23789560931164|    11.490148490289625|     7.7457818689251345|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|488.6666666666667|548.6666666666666|548.6666666666666|563.6666666666666|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2015-12-31 21:00:...| 170.0141386667267|   292.4471663647502|      86.0653955365305|     50.314535427527936| 165.41723263438084|   278.12231772796457|      98.87594592461198|      49.100686634977116|13.812544186838844| 56.224636185244115|    9.605933766887967|     6.294693994764171|15.402539242532207|   78.42674708913566|    12.387032843966068|      7.822982220380671|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            488.0|            548.0|            548.0|            563.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "|      114|2015-12-31 18:00:...|170.42359844411342|   334.3696511783739|     99.07688677874042|     48.853143551499244|  166.3251929489545|    286.4962812101407|     101.27560815245654|      47.541499460975025|  8.73668143235762|  44.16549629157097|    5.827054526494244|     6.890586104508928|15.088084779349089|   81.86613174940224|    11.384795095243218|      8.422065747978259|                     0.0|                     0.0|                     0.0|                     0.0|                     0.0|            488.0|            548.0|            548.0|            563.0|model1| 18|    (3,[],[])|     0.0|    0.0|\n",
      "+---------+--------------------+------------------+--------------------+----------------------+-----------------------+-------------------+---------------------+-----------------------+------------------------+------------------+-------------------+---------------------+----------------------+------------------+--------------------+----------------------+-----------------------+------------------------+------------------------+------------------------+------------------------+------------------------+-----------------+-----------------+-----------------+-----------------+------+---+-------------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Feature engineering final dataset files loaded!\n",
      "CPU times: user 8.78 s, sys: 3.48 s, total: 12.3 s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# load the previous created final dataset into the workspace\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# define parameters \n",
    "ACCOUNT_NAME = \"pdmvienna\"\n",
    "ACCOUNT_KEY = \"PDuXK61GpmMVWMrWdvr29THbPdlOXa61fN5RfgQV/jBO8berC1zLzZ678Nxrx+D3CRp4+ZvSff9al+lrUh8qUQ==\"\n",
    "CONTAINER_NAME = \"featureengineering\"\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# create a local path where to store the results later.\n",
    "LOCAL_DIRECT = 'model_operationalize.parquet'\n",
    "if not os.path.exists(LOCAL_DIRECT):\n",
    "    os.makedirs(LOCAL_DIRECT)\n",
    "    print('DONE creating a local directory!')\n",
    "\n",
    "# define your blob service     \n",
    "my_service = BlockBlobService(account_name=ACCOUNT_NAME, account_key=ACCOUNT_KEY)\n",
    "\n",
    "# download the entire parquet result folder to local path for a new run \n",
    "for blob in my_service.list_blobs(CONTAINER_NAME):\n",
    "    if 'featureengineering_files.parquet' in blob.name:\n",
    "        local_file = os.path.join(LOCAL_DIRECT, os.path.basename(blob.name))\n",
    "        my_service.get_blob_to_path(CONTAINER_NAME, blob.name, local_file)\n",
    "\n",
    "data = spark.read.parquet('model_operationalize.parquet')\n",
    "#data.persist()\n",
    "data.show(5)\n",
    "print('Feature engineering final dataset files loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the features, labels for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define list of input columns for downstream modeling \n",
    "input_features = [\n",
    "'volt_rollingmean_3',\n",
    "'rotate_rollingmean_3',\n",
    "'pressure_rollingmean_3',\n",
    "'vibration_rollingmean_3',\n",
    "'volt_rollingmean_24',\n",
    "'rotate_rollingmean_24',\n",
    "'pressure_rollingmean_24',\n",
    "'vibration_rollingmean_24',\n",
    "'volt_rollingstd_3',\n",
    "'rotate_rollingstd_3',\n",
    "'pressure_rollingstd_3',\n",
    "'vibration_rollingstd_3',\n",
    "'volt_rollingstd_24',\n",
    "'rotate_rollingstd_24',\n",
    "'pressure_rollingstd_24',\n",
    "'vibration_rollingstd_24',\n",
    "'error1sum_rollingmean_24',\n",
    "'error2sum_rollingmean_24',\n",
    "'error3sum_rollingmean_24',\n",
    "'error4sum_rollingmean_24',\n",
    "'error5sum_rollingmean_24',\n",
    "'comp1sum',\n",
    "'comp2sum',\n",
    "'comp3sum',\n",
    "'comp4sum',\n",
    "'age'  \n",
    "]\n",
    "\n",
    "label_var = ['label_e']\n",
    "key_cols =['machineID','dt_truncated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assemble features\n",
    "va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "data = va.transform(data).select('machineID','dt_truncated','label_e','features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set maxCategories so features with > 10 distinct values are treated as continuous.\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                               outputCol=\"indexedFeatures\", \n",
    "                               maxCategories=10).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit on whole dataset to include all labels in index\n",
    "labelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2174000\n",
      "747000\n"
     ]
    }
   ],
   "source": [
    "# split the data into train/test based on date\n",
    "training = data.filter(data.dt_truncated > \"2015-01-01\").filter(data.dt_truncated < \"2015-09-30\")\n",
    "testing = data.filter(data.dt_truncated > \"2015-09-30\")\n",
    "\n",
    "print(training.count())\n",
    "print(testing.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train a RandomForest model\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
    "\n",
    "# chain indexers and forest in a Pipeline\n",
    "pipeline_rf = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n",
    "\n",
    "# train model\n",
    "model_rf = pipeline_rf.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model and dataset schema locally first. For this ensure that the user changes the setting within aml_config and set docker.compute file to have sharedVolumes: true and prepare the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model_rf.write().overwrite().save(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdmrfull.model\r\n"
     ]
    }
   ],
   "source": [
    "# check to see if the model was saved in the shared location\n",
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authoring Realtime Web Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show the user how to author a realtime web service that scores the model you saved above. First, check to ensure that the latest version of the azure-ml-api-sdk is available for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/mmlspark/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/mmlspark/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Collecting azure-ml-api-sdk\n",
      "  Downloading azure_ml_api_sdk-0.1.0a11-py2.py3-none-any.whl (80kB)\n",
      "\u001b[K    100% |################################| 81kB 745kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): pytz in /home/mmlspark/lib/conda/lib/python3.5/site-packages (from azure-ml-api-sdk)\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /home/mmlspark/lib/conda/lib/python3.5/site-packages (from azure-ml-api-sdk)\n",
      "Collecting liac-arff (from azure-ml-api-sdk)\n",
      "  Downloading liac-arff-2.1.1.tar.gz\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /home/mmlspark/lib/conda/lib/python3.5/site-packages (from python-dateutil->azure-ml-api-sdk)\n",
      "Installing collected packages: liac-arff, azure-ml-api-sdk\n",
      "  Running setup.py install for liac-arff ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed azure-ml-api-sdk-0.1.0a11 liac-arff-2.1.1\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ml-api-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define init and run functions\n",
    "Start by defining the init() and run() functions as shown in the cell below. Then write them to the score.py file. This file will load the model, perform the prediction, and return the result.\n",
    "\n",
    "The init() function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model. This command is run when the Docker container containing your service initializes.\n",
    "The run() function defines what is executed on a scoring call. In our simple example, we simply load in the input as a data frame, and run our pipeline on the input, and return the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    \n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        input_features = [\n",
    "            'volt_rollingmean_3',\n",
    "            'rotate_rollingmean_3',\n",
    "            'pressure_rollingmean_3',\n",
    "            'vibration_rollingmean_3',\n",
    "            'volt_rollingmean_24',\n",
    "            'rotate_rollingmean_24',\n",
    "            'pressure_rollingmean_24',\n",
    "            'vibration_rollingmean_24',\n",
    "            'volt_rollingstd_3',\n",
    "            'rotate_rollingstd_3',\n",
    "            'pressure_rollingstd_3',\n",
    "            'vibration_rollingstd_3',\n",
    "            'volt_rollingstd_24',\n",
    "            'rotate_rollingstd_24',\n",
    "            'pressure_rollingstd_24',\n",
    "            'vibration_rollingstd_24',\n",
    "            'error1sum_rollingmean_24',\n",
    "            'error2sum_rollingmean_24',\n",
    "            'error3sum_rollingmean_24',\n",
    "            'error4sum_rollingmean_24',\n",
    "            'error5sum_rollingmean_24',\n",
    "            'comp1sum',\n",
    "            'comp2sum',\n",
    "            'comp3sum',\n",
    "            'comp4sum',\n",
    "            'age',\n",
    "        ]\n",
    "        \n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create schema and schema file\n",
    "Create a schema for the input to the web service and generate the schema file. This will be used to create a Swagger file for your web service which can be used to discover its input and sample data when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the input data frame\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, data.drop(\"dt_truncated\",\"failure1\",\"label_e\", \"model\",\"model_encoded\"))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': {'input_df': {'internal': {'fields': [{'metadata': {}, 'type': 'integer', 'name': 'machineID', 'nullable': True}, {'metadata': {'ml_attr': {'num_attrs': 26, 'attrs': {'numeric': [{'idx': 0, 'name': 'volt_rollingmean_3'}, {'idx': 1, 'name': 'rotate_rollingmean_3'}, {'idx': 2, 'name': 'pressure_rollingmean_3'}, {'idx': 3, 'name': 'vibration_rollingmean_3'}, {'idx': 4, 'name': 'volt_rollingmean_24'}, {'idx': 5, 'name': 'rotate_rollingmean_24'}, {'idx': 6, 'name': 'pressure_rollingmean_24'}, {'idx': 7, 'name': 'vibration_rollingmean_24'}, {'idx': 8, 'name': 'volt_rollingstd_3'}, {'idx': 9, 'name': 'rotate_rollingstd_3'}, {'idx': 10, 'name': 'pressure_rollingstd_3'}, {'idx': 11, 'name': 'vibration_rollingstd_3'}, {'idx': 12, 'name': 'volt_rollingstd_24'}, {'idx': 13, 'name': 'rotate_rollingstd_24'}, {'idx': 14, 'name': 'pressure_rollingstd_24'}, {'idx': 15, 'name': 'vibration_rollingstd_24'}, {'idx': 16, 'name': 'error1sum_rollingmean_24'}, {'idx': 17, 'name': 'error2sum_rollingmean_24'}, {'idx': 18, 'name': 'error3sum_rollingmean_24'}, {'idx': 19, 'name': 'error4sum_rollingmean_24'}, {'idx': 20, 'name': 'error5sum_rollingmean_24'}, {'idx': 21, 'name': 'comp1sum'}, {'idx': 22, 'name': 'comp2sum'}, {'idx': 23, 'name': 'comp3sum'}, {'idx': 24, 'name': 'comp4sum'}, {'idx': 25, 'name': 'age'}]}}}, 'type': {'pyClass': 'pyspark.ml.linalg.VectorUDT', 'sqlType': {'fields': [{'metadata': {}, 'type': 'byte', 'name': 'type', 'nullable': False}, {'metadata': {}, 'type': 'integer', 'name': 'size', 'nullable': True}, {'metadata': {}, 'type': {'containsNull': False, 'elementType': 'integer', 'type': 'array'}, 'name': 'indices', 'nullable': True}, {'metadata': {}, 'type': {'containsNull': False, 'elementType': 'double', 'type': 'array'}, 'name': 'values', 'nullable': True}], 'type': 'struct'}, 'type': 'udt', 'class': 'org.apache.spark.ml.linalg.VectorUDT'}, 'name': 'features', 'nullable': True}], 'type': 'struct'}, 'type': 2, 'swagger': {'items': {'type': 'object', 'properties': {'features': {'type': 'object'}, 'machineID': {'format': 'int32', 'type': 'integer'}}}, 'type': 'array', 'example': [{'features': '[163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]', 'machineID': 114}, {'features': '[168.056072384,279.969575347,97.6337282091,48.2422167954,164.03977789,265.414030135,98.269271327,51.5743666555,17.8206438245,49.6374290039,9.78930475607,7.96334441364,15.4885626934,67.263988252,10.3431460885,6.73810962767,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]', 'machineID': 114}, {'features': '[161.596232787,277.717144157,95.714207592,51.5081825346,164.567780475,265.993420048,99.6572121486,50.4624849087,20.5822674785,80.9773971021,8.12863049884,10.0460367966,15.5511653545,65.2378956093,11.4901484903,7.74578186893,0.0,0.0,0.0,0.0,0.0,488.666666667,548.666666667,548.666666667,563.666666667,18.0]', 'machineID': 114}]}}}}\n"
     ]
    }
   ],
   "source": [
    "x = generate_schema(run_func=run, inputs=inputs, filepath='service_schema.json')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test init and run\n",
    "We can then test the init() and run() functions right here in the notebook, before we decide to actually publish a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[114,\n",
       "  163.375732902,\n",
       "  333.149484586,\n",
       "  100.183951698,\n",
       "  44.0958812638,\n",
       "  164.114723991,\n",
       "  277.191815232,\n",
       "  97.6289110707,\n",
       "  50.8853505161,\n",
       "  21.0049565219,\n",
       "  67.5287259378,\n",
       "  12.9361526861,\n",
       "  4.61359760918,\n",
       "  15.5377738062,\n",
       "  67.6519885441,\n",
       "  10.528274633,\n",
       "  6.94129487555,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  489.0,\n",
       "  549.0,\n",
       "  549.0,\n",
       "  564.0,\n",
       "  18.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how the input data should be\n",
    "input_data = [[114, 163.375732902,333.149484586,100.183951698,44.0958812638,164.114723991,277.191815232,97.6289110707,50.8853505161,21.0049565219,67.5287259378,12.9361526861,4.61359760918,15.5377738062,67.6519885441,10.528274633,6.94129487555,0.0,0.0,0.0,0.0,0.0,489.0,549.0,549.0,564.0,18.0]]\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = (spark.createDataFrame(input_data, [\"machineID\", \"volt_rollingmean_3\", \"rotate_rollingmean_3\", \"pressure_rollingmean_3\", \"vibration_rollingmean_3\", \"volt_rollingmean_24\", \n",
    "            \"rotate_rollingmean_24\", \"pressure_rollingmean_24\", \"vibration_rollingmean_24\", \"volt_rollingstd_3\", \"rotate_rollingstd_3\",\n",
    "            \"pressure_rollingstd_3\", \"vibration_rollingstd_3\", \"volt_rollingstd_24\", \"rotate_rollingstd_24\", \"pressure_rollingstd_24\",\n",
    "            \"vibration_rollingstd_24\", \"error1sum_rollingmean_24\", \"error2sum_rollingmean_24\", \"error3sum_rollingmean_24\",\n",
    "            \"error4sum_rollingmean_24\", \"error5sum_rollingmean_24\", \"comp1sum\", \"comp2sum\", \"comp3sum\", \"comp4sum\",\n",
    "            \"age\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test init() in local notebook\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0.0\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"0.0\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test run() in local notebook\n",
    "run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the schema file for deployment\n",
    "out = json.dumps(x)\n",
    "with open(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY'] + 'service_schema.json', 'w') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdmrfull.model\tservice_schema.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user will need to navigate to the folder: \n",
    "```C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name> ```\n",
    "\n",
    "Copy the file service_schema.json to your projects folder for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use %%writefile command will save the *.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /azureml-share/pdmscore.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /azureml-share/pdmscore.py\n",
    "# after testing the below init() and run() functions,\n",
    "# uncomment this cell to create the score.py after.\n",
    "\n",
    "# remove import from init() from function.\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import concat, col, udf, lag, date_add, explode, lit, unix_timestamp\n",
    "from pyspark.sql.functions import month, weekofyear, dayofmonth\n",
    "from pyspark.sql.functions import datediff, to_date, lit, unix_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.dataframe import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.feature import StandardScaler, PCA, RFormula\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema\n",
    "\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']+'pdmrfull.model')\n",
    "    \n",
    "def run(input_df):\n",
    "    import json\n",
    "    response = ''\n",
    "    \n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        input_features = [\n",
    "            'volt_rollingmean_3',\n",
    "            'rotate_rollingmean_3',\n",
    "            'pressure_rollingmean_3',\n",
    "            'vibration_rollingmean_3',\n",
    "            'volt_rollingmean_24',\n",
    "            'rotate_rollingmean_24',\n",
    "            'pressure_rollingmean_24',\n",
    "            'vibration_rollingmean_24',\n",
    "            'volt_rollingstd_3',\n",
    "            'rotate_rollingstd_3',\n",
    "            'pressure_rollingstd_3',\n",
    "            'vibration_rollingstd_3',\n",
    "            'volt_rollingstd_24',\n",
    "            'rotate_rollingstd_24',\n",
    "            'pressure_rollingstd_24',\n",
    "            'vibration_rollingstd_24',\n",
    "            'error1sum_rollingmean_24',\n",
    "            'error2sum_rollingmean_24',\n",
    "            'error3sum_rollingmean_24',\n",
    "            'error4sum_rollingmean_24',\n",
    "            'error5sum_rollingmean_24',\n",
    "            'comp1sum',\n",
    "            'comp2sum',\n",
    "            'comp3sum',\n",
    "            'comp4sum',\n",
    "            'age',\n",
    "        ]\n",
    "\n",
    "        va = VectorAssembler(inputCols=(input_features), outputCol='features')\n",
    "        data = va.transform(input_df).select('machineID','features')\n",
    "        score = pipeline.transform(data)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        preds = [str(x['prediction']) for x in predictions]\n",
    "        response = \",\".join(preds)\n",
    "    except Exception as e:\n",
    "        print(\"Error: {0}\",str(e))\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    print(json.dumps(response))\n",
    "    return json.dumps(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init()\n",
    "    run(\"{\\\"input_df\\\":[{\\\"machineID\\\":114,\\\"volt_rollingmean_3\\\":163.375732902,\\\"rotate_rollingmean_3\\\":333.149484586,\\\"pressure_rollingmean_3\\\":100.183951698,\\\"vibration_rollingmean_3\\\":44.0958812638,\\\"volt_rollingmean_24\\\":164.114723991,\\\"rotate_rollingmean_24\\\":277.191815232,\\\"pressure_rollingmean_24\\\":97.6289110707,\\\"vibration_rollingmean_24\\\":50.8853505161,\\\"volt_rollingstd_3\\\":21.0049565219,\\\"rotate_rollingstd_3\\\":67.5287259378,\\\"pressure_rollingstd_3\\\":12.9361526861,\\\"vibration_rollingstd_3\\\":4.61359760918,\\\"volt_rollingstd_24\\\":15.5377738062,\\\"rotate_rollingstd_24\\\":67.6519885441,\\\"pressure_rollingstd_24\\\":10.528274633,\\\"vibration_rollingstd_24\\\":6.94129487555,\\\"error1sum_rollingmean_24\\\":0.0,\\\"error2sum_rollingmean_24\\\":0.0,\\\"error3sum_rollingmean_24\\\":0.0,\\\"error4sum_rollingmean_24\\\":0.0,\\\"error5sum_rollingmean_24\\\":0.0,\\\"comp1sum\\\":489.0,\\\"comp2sum\\\":549.0,\\\"comp3sum\\\":549.0,\\\"comp4sum\\\":564.0,\\\"age\\\":18.0}]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pdmrfull.model\tpdmscore.py  service_schema.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls $AZUREML_NATIVE_SHARE_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the user will need to navigate to the folder: \n",
    "```C:\\Users\\<username>\\.azureml\\share\\<team account>\\<Project Name> ```\n",
    "\n",
    "Copy the file pdmscore.py to your projects folder for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the CLI to deploy and manage your web service \n",
    "\n",
    "## Pre-requisites \n",
    "\n",
    "Use the following commands to set up an environment and account to run the web service. For more info, see the Getting Started Guide and the CLI Command Reference. You can use -h flag at the end of the commands for command help.\n",
    "\n",
    "• Create the environment (you need to do this once per environment e.g. dev or prod)\n",
    "```\n",
    "az ml env setup -c -n <yourclustername> --location <e.g. eastus2>\n",
    "```\n",
    "\n",
    "• Create a Model Management account (one time setup)\n",
    "```\n",
    "az ml account modelmanagement create --location <e.g. eastus2> -n <your-new-acctname> -g <yourresourcegroupname> --sku-instances 1 --sku-name S1\n",
    "```\n",
    "\n",
    "• Set the Model Management account\n",
    "```\n",
    "az ml account modelmanagement set -n <youracctname> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "• Set the environment. The cluster name is the name used in step 1 above. The resource group name was the output of the same process and would be in the command window when the setup process is completed.\n",
    "```\n",
    "az ml env set -n <yourclustername> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "## Deploy your web service \n",
    "\n",
    "Switch to a bash shell, and run the following commands to deploy your service and run it.\n",
    "\n",
    "Enter the path where the notebook and other files are saved. Your actual path may be different from this example.\n",
    "```\n",
    "cd ~/notebooks/azureml/realtime/\n",
    "```\n",
    "\n",
    "This assumes that you saved your model locally.\n",
    "```\n",
    "az ml service create realtime -f pdmscore.py -r  spark-py -m pdmrfull.model -s service_schema.json -n pdmservice --cpu 0.1\n",
    "```\n",
    "\n",
    "This command will return the sample run command with sample data. You can get the Service Id from the output of the create command above.\n",
    "```\n",
    "az ml service show realtime -i <yourserviceid>\n",
    "```\n",
    "\n",
    "Call the web service to get a prediction\n",
    "```\n",
    "az ml service run realtime -i <yourserviceid> -d \"{\\\"input_df\\\": [{\\\"machineID\\\":114, \\\"vo\n",
    "lt_rollingmean_3\\\":163.375732902, \\\"rotate_rollingmean_3\\\":333.149484586, \\\"pressure_rollingmean_3\\\":100.183951698, \\\"vibration_rollingmean_3\\\":44.0958812638, \\\"volt_rollingme\n",
    "an_24\\\":164.114723991, \\\"rotate_rollingmean_24\\\":277.191815232, \\\"pressure_rollingmean_24\\\":97.6289110707, \\\"vibration_rollingmean_24\\\":50.8853505161, \\\"volt_rollingstd_3\\\":21\n",
    ".0049565219, \\\"rotate_rollingstd_3\\\":67.5287259378, \\\"pressure_rollingstd_3\\\":12.9361526861, \\\"vibration_rollingstd_3\\\":4.61359760918, \\\"volt_rollingstd_24\\\":15.5377738062, \\\"\n",
    "rotate_rollingstd_24\\\":67.6519885441, \\\"pressure_rollingstd_24\\\":10.528274633, \\\"vibration_rollingstd_24\\\":6.94129487555, \\\"error1sum_rollingmean_24\\\":0.0, \\\"error2sum_rolling\n",
    "mean_24\\\":0.0, \\\"error3sum_rollingmean_24\\\":0.0, \\\"error4sum_rollingmean_24\\\":0.0, \\\"error5sum_rollingmean_24\\\":0.0, \\\"comp1sum\\\":489.0, \\\"comp2sum\\\":549.0, \\\"comp3sum\\\":549.0\n",
    ", \\\"comp4sum\\\":564.0, \\\"age\\\":180}]}\"\n",
    "```\n",
    "\n",
    "Predicted output label is as follows:\n",
    "```\n",
    "\"0.0\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PredictiveMaintenanceScenario docker",
   "language": "python",
   "name": "predictivemaintenancescenario_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
